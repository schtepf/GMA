\documentclass[a4paper]{article}

\usepackage{vmargin}
\setpapersize[portrait]{A4}
\setmarginsrb{30mm}{10mm}{30mm}{20mm}% left, top, right, bottom
{12pt}{15mm}% head heigth / separation
{0pt}{15mm}% bottom height / separation
%% \setmargnohfrb{30mm}{20mm}{20mm}{20mm}

\setlength{\parindent}{0mm}
\setlength{\parskip}{\medskipamount}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage[T1]{fontenc}
%% \usepackage{textcomp}  % this can break some outline symbols in CM fonts, use only if absolutely necessary

\usepackage{lmodern}   % type1 computer modern fonts in T1 encoding
%% \usepackage{mathptmx}  % use Adobe Times as standard font with simulated math support
%% \usepackage{mathpazo}  % use Adobe Palatino as standard font with simulated math support

%% \usepackage{pifont}
%% \usepackage{eucal}
\usepackage{mathrsfs} % \mathscr

\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx,rotating}
\usepackage{array,hhline,booktabs}
\usepackage{xspace}
\usepackage{url,hyperref}
%% \usepackage{ifthen,calc,hyphenat}
\usepackage{enumitem}
\setlist{noitemsep}

\usepackage{xcolor,tikz}
\usepackage[textwidth=25mm,textsize=footnotesize,colorinlistoftodos,backgroundcolor=orange!80]{todonotes} % [disable] to hide all TODOs

\usepackage{natbib}
\bibpunct[:~]{(}{)}{;}{a}{}{,}

\input{lib/math.tex}
\input{lib/text.tex}
\input{lib/stat.tex}
\input{lib/vector.tex}

\title{The mathematics of Geometric Multivariate Analysis}
\author{Stephanie Evert}
\date{7 July 2024}

\begin{document}
\maketitle

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Linear discriminant analysis}
\label{sec:lda}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Background material}
\label{sec:lda:background}

\begin{itemize}
\item originally proposed by \citet{Fisher:36} for a one-dimensional discriminant between two groups
  \begin{itemize}
  \item uses $D^2 / S$ as separation criterion where $D$ is the difference between the group means and $S$ the within group variance (computed from within-group covariance matrix $\mathbf{S}$)
  \item directly solves for minimum, resulting in equation system $\mathbf{S} \boldsymbol{\lambda} = \vd$
  \item Fisher does not discuss an extension to multiple groups (using between-group variance as criterion) nor to a multi-dimensional discriminant
  \end{itemize}
\item data matrix $\mathbf{X}\in \setR^{n\times d}$ with $n$ data points $\vx_i\in \setR^d$
\item LDA algorithm as implemented in the \texttt{MASS} package is described by \citet[331--332]{Venables:Ripley:02}:
  \begin{itemize}
  \item matrix of group means $\mathbf{M}\in \setR^{g\times d}$ as row vectors $\vm_j$
  \item group indicator matrix $\mathbf{G}\in \setR^{n\times g}$ with $g_{ij} = 1$ iff $X_i$ belongs to group $j$
  \item $\overline{\vx}\in \setR^d$ the overall mean $\overline{\vx} = \frac1n \sum_i \vx_i$
  \item the ``group predictions'' are given by $\mathbf{G}\mathbf{M}$
  \item within-group covariance matrix $\mathbf{W}$ and between-group covariance matrix $\mathbf{B}$ are
    \begin{equation}
      \label{eq:lda:mass-W-B}
      \mathbf{W} = \frac{
        (\mathbf{X} - \mathbf{GM})\T (\mathbf{X} - \mathbf{GM})
      }{ n - g }
      ,\qquad
      \mathbf{B} = \frac{
        (\mathbf{GM} - \vone \overline{\vx}\T)\T (\mathbf{GM} - \vone \overline{\vx}\T)
      }{g - 1}
    \end{equation}
  \item a one-dimensional discriminant is given by a linear combination $\va\T \vx$ that maximises the ratio of between-group to within-group variance along the discriminant axis:
    \begin{equation}
      \label{eq:lda:mass-criterion}
      \frac{\va\T \mathbf{B} \va}{\va\T \mathbf{W} \va}
    \end{equation}
  \item NB: this criterion is proportional to the F-statistic of ANOVA; since it differs only by a fixed factor, the choice of $\va$ also maximises the F-statistic%
    \footnote{See Wikipedia article on \href{https://en.wikipedia.org/wiki/Analysis_of_variance\#The_F-test}{Analysis of variance} for the usual form of the F-statistic. See Wikipedia articles on the \href{https://en.wikipedia.org/wiki/F-test\#Formula_and_calculation}{F-test} and the \href{https://en.wikipedia.org/wiki/F-distribution\#Definition}{F-distribution} for an explanation of the scaling factors involved.}
  \item to find the maximum, compute a sphering $\vy = \mathbf{S} \vx$ of the variables so that the within-group covariance matrix becomes $\mathbf{W}' = \mathbf{I}$
  \item the problem is then to maximise $\va\T \mathbf{B}' \va$ for the transformed between-group matrix $\mathbf{B}$ subject to $\norm{\va} = 1$ (because the transformation $\va' = \mathbf{S}^{-1} \va$ yields the same value for (\ref{eq:lda:mass-criterion}))
  \item $\va$ is then easily found as the largest principal component of $\mathbf{B}'$
  \item for an extension to a multi-dimensional discriminant, the first $r$ principal components can be used, and the number of dimensions can be chosen according to their principal values or $R^2$; while this is plausible in the sphered coordinates, Venables \& Ripley don't explain what separation criterion it optimises in the original coordinate system
  \end{itemize}
\item a different explanation of the LDA algorithm is given by \citet[186--190]{Bishop:06}, who explicitly discusses the extension to multiple classes and a multi-dimensional discriminant \citep[191--192]{Bishop:06}
\item Bishop also points out the problem that it is no longer clear which separation criterion should be maximised and refers to \citet[445--459]{Fukunaga:90} for a detailed exposition of different criteria and their optimisation
\end{itemize}

\paragraph{Useful Wikipedia articles}

\begin{itemize}
\item Analysis of variance: \url{https://en.wikipedia.org/wiki/Analysis_of_variance}
\item F-test: \url{https://en.wikipedia.org/wiki/F-test#Formula_and_calculation}
\item F-distribution: \url{https://en.wikipedia.org/wiki/F-distribution#Definition}
\item MANOVA separation criteria: \url{https://en.wikipedia.org/wiki/Multivariate_analysis_of_variance#Hypothesis_Testing}
\item Linear discriminant analysis: \url{https://en.wikipedia.org/wiki/Linear_discriminant_analysis}, esp.\ \url{https://en.wikipedia.org/wiki/Linear_discriminant_analysis#Multiclass_LDA}
\item Blessing of dimensionality: \url{https://en.wikipedia.org/wiki/Curse_of_dimensionality#Blessing_of_dimensionality} (but more relevant for Azuma paper)
\end{itemize}

\paragraph{Other material}

\begin{itemize}
\item Implementation of \texttt{lda()} in \url{https://github.com/cran/MASS/blob/master/R/lda.R}%
  \footnote{local copy in \url{file:///Users/ex47emin/Software/R/MASS-GIT/R/lda.R}}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Analysis of variance}
\label{sec:lda:anova}

Unsurprisingly, LDA \citep{Fisher:36} is closely connected to the analysis of variance or \textbf{ANOVA} \citep{Fisher:25}. We start by summarising the ANOVA method following the exposition in \citet[754--761]{DeGroot:Schervish:12}, but with modified notation.

\begin{itemize}
\item data: $n$ observations $y_i\in \setR$ belonging to $g$ groups; $g_i\in \set{1,\ldots,g}$ indicates group membership of $y_i$; group sizes are given by $n_j = \abs{\set{g_i = j}} = \sum_{g_i = j} 1$
\item assumptions: items of group $j$ are i.i.d.\ samples from normal distribution $N(\mu_j, \sigma^2)$; variance $\sigma^2$ is equal for all groups, but the group means $\mu_j$ may be different
\item ANOVA null hypothesis to be tested is $H_0: \mu_1 = \ldots = \mu_g$ (equal group means)
\item observed overall mean $m$ and group means $m_j$ are given by
  \begin{equation}
    \label{eq:lda:anova:means}
    m = \frac1n \sum_{i=1}^n y_i \qquad
    m_j = \frac1{n_j} \sum_{g_i = j} y_i
  \end{equation}
\item basic idea: \textbf{sum of squares} as measure of variability of the data set can be partitioned into within-group and between-group components: $S^2 = S^2_W + S^2_B$ \citep[758]{DeGroot:Schervish:12}
  \begin{align*}
    S^2 &= \sum_{i=1}^n (y_i - m)^2 \\
    S^2_W &= \sum_{j=1}^g \sum_{g_i = j} (y_i - m_j)^2 = \sum_{i=1}^n (y_i - m_{g_i})^2 \\
    S^2_B &= \sum_{j=1}^g n_j (m_j - m)^2 = \sum_{i=1}^n (m_{g_i} - m)^2
  \end{align*}
\item $S^2_W / \sigma^2$ has a $\chi^2_{n-g}$ distribution \citep[757]{DeGroot:Schervish:12}; it follows that the \textbf{within-group variance} $W$ is an unbiased estimator of $\sigma^2$
  \begin{equation}
    \label{eq:lda:anova:W}
    W = \frac{ \sum_{i=1}^n (y_i - m_{g_i})^2 }{ n - g }
  \end{equation}
\item under $H_0$ it can be shown that $S^2_B / \sigma^2$ has a $\chi^2_{g-1}$ distribution \citep[759]{DeGroot:Schervish:12}%
  \footnote{note that under $H_0$ we have $m_j \sim N(\mu, \sigma^2 / n_j)$} %
  and the \textbf{between-group variance} $B$ is also an unbiased estimator of $\sigma^2$
  \begin{equation}
    \label{eq:lda:anova:B}
    B = \frac{ \sum_{j=1}^g n_j (m_j - m)^2 }{ g - 1 }
  \end{equation}
\item if $H_0$ does not hold, we expect $B$ to be larger than $\sigma^2$ (because of the added variability between the group means $\mu_j$) so that the ratio
  \begin{equation}
    \label{eq:lda:anova:U}
    F = \frac{B}{W} = \frac{S^2_B / (g - 1)}{S^2_W / (n - g)}
  \end{equation}
  is a suitable test statistic for ANOVA; p-values can be obtained from its $F_{g-1, n-g}$ distribution under $H_0$ \citep[759]{DeGroot:Schervish:12}
\end{itemize}

Analysis of variance can be generalised to a comparison of group means for multivariate data (\textbf{MANOVA}). Many concepts carry over in a straightforward way, but a suitable test statistic and its sampling distribution under $H_0$ are less obvious. The summary shown here is based on the Wikipedia article \href{https://en.wikipedia.org/wiki/Multivariate_analysis_of_variance}{\emph{Multivariate analysis of variance}}, again with modified notation.

\begin{itemize}
\item data are vectors $\vy_i\in \setR^d$ with group membership $g_i$
\item assumption: each group $j$ has a multivariate normal distribution $N(\vmu_j, \matSigma)$ with equal covariance matrix $\matSigma$, but possibly different group means $\vmu_j$
\item MANOVA null hypothesis $H_0: \vmu_1 = \ldots = \vmu_g$
\item overall mean $\vm$ and group means $\vm_j$ are
  \begin{equation}
    \label{eq:lda:manova:means}
    \vm = \frac1n \sum_{i=1}^n \vy_i \qquad
    \vm_j = \frac1{n_j} \sum_{g_i = j} \vy_i
  \end{equation}
\item instead of a sum of squares, we partition the \textbf{covariance matrix} $\mathbf{C}$ given by
  \begin{equation}
    \label{eq:lda:manova:C}
    \mathbf{C} = \frac1{n-1} \sum_{i=1}^n (\vy_i - \vm) (\vy_i - \vm)\T
  \end{equation}
  where the transpose cross-product computes all squares and products of $\vy_i - \vm$
\item we partition $\mathbf{C}$ into within-group and between-group covariance matrices in the form
  \[
    (n-1) \mathbf{C} = (n - g) \mathbf{W} + (g - 1) \mathbf{B}
  \]
  with
  \begin{align}
    \label{eq:lda:manova:W}
    \mathbf{W} &= \frac1{n - g} \sum_{i=1}^n (\vy_i - \vm_{g_i}) (\vy_i - \vm_{g_i})\T \\
    \label{eq:lda:manova:B}
    \mathbf{B} &= \frac1{g - 1} \sum_{j=1}^g n_j (\vm_j - \vm) (\vm_j - \vm)\T
  \end{align}
  \citep[cf.][191--192]{Bishop:06}
\item according to the Wikipedia article \href{https://en.wikipedia.org/wiki/Multivariate_normal_distribution\#Parameter_estimation}{\emph{Multivariate normal distribution}}\footnote{but [\emph{citation needed}]} $\mathbf{C}$ is an unbiased estimator of $\matSigma$ under $H_0$; correspondingly, $\mathbf{W}$ is always an unbiased estimator of $\matSigma$ and $\mathbf{B}$ is under $H_0$
\item this motivates $\mathbf{A} = \mathbf{B} \mathbf{W}^{-1}$ as a widely-used test criterion with $\mathbf{A}\approx \mathbf{I}$ under $H_0$; intuitively, $\mathbf{A}$ compares the shape and magnitude of the between-group covariance matrix against the within-group covariance matrix; it should, in particular, also detected cases where there are unexpectedly large differences between group means along an axis that has small within-group variance
\item the precise choice of a test statistic is less obvious; common options include Wilks's lambda $\lambda_{\text{Wilks}} = \Det{\mathbf{I} + \mathbf{A}}^{-1}$ and the Lawley-Hotelling trace $\lambda_{\text{LH}} = \Trace{\mathbf{A}}$
\item exact distributions of these test statistics under $H_0$ are not available, except for $g = 2$, where they reduce to Hotelling's $t^2$ distribution\footnote{but [\emph{citation needed}]}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The LDA algorithm}
\label{sec:lda:standard}

\subsubsection{Data set and goals of LDA}
\label{sec:lda:standard:goals}

\begin{itemize}
\item data are $n$ feature vectors $\vx_i \in \setR^d$ combined into a data matrix $\mathbf{X}\in \setR^{n\times d}$
\item each data point is assigned to one of $g$ groups indicated by $g_i\in \set{1, \ldots, g}$; the sizes of the groups are $n_j = \abs{\set{g_i = j}}$
\item LDA aims to find a one-dimensional projection (the \textbf{discriminant}) that maximises the separation between groups
\item \citet{Fisher:36} and most textbooks introduce LDA for the special case $g = 2$ of two groups, for which an optimal discriminant can easily be derived; we formulate its generalisation to an arbitrary number of groups based on the $F$ statistic of ANOVA%
  \footnote{our approach implicitly builds on the same distributional assumptions as ANOVA, which motivate the use of the $F$ statistic as an optimality criterion; they are not a necessary pre-requisite for application of the LDA method, but results will be most sensible if $\matSigma$ is roughly equal across all groups}
\item \textbf{task}: find axis $\va\in \setR^d$ that maximises the $F$ statistic of discriminant scores $y_i = \va\T \vx_i$
\end{itemize}

\subsubsection{Covariance matrix and projection}
\label{sec:lda:standard:covmat}

\begin{itemize}
\item this more explicit derivation corresponds to the LDA algorithm described by \citet[331--332]{Venables:Ripley:02} and thus to (one variant of) its implementation in the MASS package
\item overall mean $\vm$ and group means $\vm_j$ are given by
  \begin{equation}
    \label{eq:lda:means}
    \vm = \frac1n \sum_{i=1}^n \vx_i \qquad
    \vm_j = \frac1{n_j} \sum_{g_i = j} \vx_i
  \end{equation}
\item within-group and between-group \textbf{covariance matrices} are defined as in (\ref{eq:lda:manova:W}) and (\ref{eq:lda:manova:B})
  \begin{align}
    \label{eq:lda:W}
    \mathbf{W} &= \frac1{n - g} \sum_{i=1}^n (\vx_i - \vm_{g_i}) (\vx_i - \vm_{g_i})\T \\
    \label{eq:lda:B}
    \mathbf{B} &= \frac1{g - 1} \sum_{j=1}^g n_j (\vm_j - \vm) (\vm_j - \vm)\T
  \end{align}
\item given an axis $\va\in \setR^d$, the one-dimensional discriminant scores of data points are $y_i = \va\T \vx_i$; due to linearity the overall and group means are $m = \va\T \vm$ and $m_j = \va\T \vm_j$
\item hence the within-group variance (\ref{eq:lda:anova:W}) can be computed as
  \begin{equation}
    \label{eq:lda:discW}
    \begin{split}
      W &= \frac1{n - g} \sum_{i=1}^n (\va\T \vx_i - \va\T \vm_{g_i})^2 \\
        &= \frac1{n - g} \sum_{i=1}^n (\va\T \vx_i - \va\T \vm_{g_i}) (\va\T \vx_i - \va\T \vm_{g_i})\T \\
        &= \frac1{n - g} \sum_{i=1}^n \va\T (\vx_i - \vm_{g_i}) (\vx_i - \vm_{g_i})\T \va \\
        &= \va\T \mathbf{W} \va
    \end{split}
  \end{equation}
\item analogously the between-group variance (\ref{eq:lda:anova:B}) can be computed as
  \begin{equation}
    \label{eq:lda:discB}
    B = \va\T \mathbf{B} \va
  \end{equation}
\item our goal is to find an axis $\va$ that maximises the test statistic $F = B / W$, so that we can most clearly reject $H_0$ of equal group means for the discriminant scores $y_i$
  \begin{equation}
    \label{eq:lda:F-stat}
    F = \frac{B}{W} = \frac{ \va\T \mathbf{B} \va }{ \va\T \mathbf{W} \va }
  \end{equation}
\end{itemize}

\subsubsection{Coordinate transformation}
\label{sec:lda:standard:sphering}

\begin{itemize}
\item a convenient approach starts by \textbf{sphering} the within-group covariance matrix $\mathbf{W}$ with a coordinate transformation $\vx' = \mathbf{S} \vx$ such that in the new coordinate system $\mathbf{W}' = \mathbf{I}$
\item the homomorphism preserves overall and group means: $\vm' = \mathbf{S} \vm$ and $\vm'_j = \mathbf{S} \vm_j$
\item the within-group covariance matrix $\mathbf{W}'$ in the new coordinate system is
  \begin{equation}
    \label{eq:lda:Wprime}
    \begin{split}
      \mathbf{W}'
      &= \frac1{n-g} \sum_{i=1}^n (\vx'_i - \vm'_{g_i}) (\vx'_i - \vm'_{g_i})\T \\
      &= \frac1{n-g} \sum_{i=1}^n (\mathbf{S}\vx_i - \mathbf{S}\vm_{g_i}) (\mathbf{S}\vx_i - \mathbf{S}\vm_{g_i})\T \\
      &= \mathbf{S} \mathbf{W} \mathbf{S}\T
    \end{split}
  \end{equation}
\item in the same way we can easily see that the between-group covariance matrix is $\mathbf{B}' = \mathbf{S} \mathbf{B} \mathbf{S}\T$
\item a suitable coordinate transformation $\mathbf{S}$ can be derived from the \textbf{eigenvalue decomposition} of the symmetric, positive semidefinite matrix $\mathbf{W} = \mathbf{U} \mathbf{D} \mathbf{U}\T$ where $\mathbf{D}$ is the diagonal matrix of eigenvalues $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_d$ and the columns of $\mathbf{U}$ are the corresponding eigenvectors; note that $\mathbf{U}$ is an orthonormal matrix, i.e.\ $\mathbf{U}^{-1} = \mathbf{U}\T$ or $\mathbf{U} \mathbf{U}\T = \mathbf{U}\T \mathbf{U} = \mathbf{I}$
\item prerequisite: $\mathbf{W}$ must be positive definite ($\lambda_d > 0$) with good condition number $\lambda_1 / \lambda_d$
\item then we can define $\mathbf{S} = \mathbf{D}^{-\frac12} \mathbf{U}\T$ with inverse transformation $\mathbf{S}^{-1} = \mathbf{U} \mathbf{D}^{\frac12}$
\item within-group covariance matrix $\mathbf{W}'$ in the transformed coordinates:
  \begin{equation}
    \label{eq:lda:Wprime-I}
    \mathbf{W}'
    = \mathbf{S} \mathbf{W} \mathbf{S}\T
    = \mathbf{D}^{-\frac12} \mathbf{U}\T ( \mathbf{U} \mathbf{D} \mathbf{U}\T ) \mathbf{U} \mathbf{D}^{-\frac12} 
    = \mathbf{D}^{-\frac12} \mathbf{D} \mathbf{D}^{-\frac12}
    = \mathbf{I}
  \end{equation}
\end{itemize}

\subsubsection{LDA discriminant}
\label{sec:lda:standard:discriminant}

\begin{itemize}
\item since the discriminant axis $\va$ describes a linear form $\vx \mapsto y = \va\T \vx$ it is subjected to the inverse transformation $(\va')\T = \va\T \mathbf{S}^{-1}$, which corresponds to the identity $\va = \mathbf{S}\T \va'$
\item confirm that the F-statistic is invariant under these transformations:
  \begin{equation}
    \label{eq:lda:F-transformation}
    F = \frac{B}{W} = \frac{ \va\T \mathbf{B} \va }{ \va\T \mathbf{W} \va }
    = \frac{ (\va')\T \mathbf{S} \mathbf{B} \mathbf{S}\T \va' }{ (\va')\T \mathbf{S} \mathbf{W} \mathbf{S}\T \va' }
    = \frac{ (\va')\T \mathbf{B}' \va' }{ (\va')\T \mathbf{W}' \va' }
    = \frac{B'}{W'}
  \end{equation}
\item it is thus sufficient to find $\va'$ that maximises $F$ in the transformed coordinates:
  \begin{equation}
    \label{eq:lda:Fprime}
    \frac{B'}{W'}
    = \frac{ (\va')\T \mathbf{B}' \va' }{ (\va')\T \mathbf{W}' \va' }
    = \frac{ (\va')\T \mathbf{B}' \va' }{ (\va')\T \va' }
    = \frac{ (\va')\T \mathbf{B}' \va' }{ \norm{\va'}^2 }    
  \end{equation}
  or equivalently maximise $(\va')\T \mathbf{B}' \va'$ under the constraint $\norm{\va'} = 1$
\item it is well-known that the solution is given by the first eigenvector $\vv_1$ of $\mathbf{B'}$; this is also easy to see: for every eigenvector $\vv_i$ we have $\norm{\vv_i} = 1$ and $\vv_i\T \mathbf{B}' \vv_i = \mu_i$ the corresponding eigenvalue, so the best choice is $\va' = \vv_1$ with the largest eigenvalue $\mu_1$
\item the optimal discriminant axis in original coordinates is thus $\va = \mathbf{S}\T \vv_1$
\end{itemize}

\subsubsection{LDA with multiple discriminants}
\label{sec:lda:standard:multiple}

\begin{itemize}
\item for $g > 2$ it is usually necessary to consider a multi-dimensional \textbf{discriminant space} (of up to $g - 1$ dimensions) to achieve an optimal separation of groups
\item we thus have multiple discriminants $\va_1, \ldots, \va_r\in \setR^d$ describing linear forms $\vx \mapsto y_k = \va_k\T\vx$, which we collect as rows of the \textbf{discriminant matrix} $\mathbf{A}\in \setR^{r\times d}$, so that $\vy = \mathbf{A} \vx \in \setR^r$
\item overall and group means in the \textbf{discriminant space} are $\tvm = \mathbf{A} \vm$ and $\tvm_j = \mathbf{A} \vm_j$ (due to linearity); within-group and between-group covariance matrices are obtained in analogy to (\ref{eq:lda:discW}) and (\ref{eq:lda:discB}) as
  \begin{equation}
    \label{eq:lda:tildeWB}
    \tmat W = \mathbf{A} \mathbf{W} \mathbf{A}\T, \qquad
    \tmat B = \mathbf{A} \mathbf{B} \mathbf{A}\T
  \end{equation}
\item for measuring separation of groups within the discriminant space we use the Lawley-Hotelling trace as a MANOVA test statistic:
  \begin{equation}
    \label{eq:lda:LH}
    \llh(\mathbf{A}) = \Trace{\tmat B \tmat W^{-1}}
  \end{equation}
   our goal is to find a discriminant matrix $\mathbf{A}$ that maximises $\llh(\mathbf{A})$
 \item a first important property of $\llh$ is its invariance under coordinate transformations in the discriminant space; for any coordinate transformation $\mathbf{S}\in \setR^{r\times r}$ we have in analogy to (\ref{eq:lda:Wprime})
   \begin{equation}
     \label{eq:lda:tildeWBtransform}
     \tmat B\mapsto \mathbf{S} \tmat B \mathbf{S}\T,\qquad
     \tmat W^{-1} \mapsto (\mathbf{S} \tmat W \mathbf{S}\T)^{-1}
     = (\mathbf{S}\T)^{-1} \tmat W^{-1} \mathbf{S}^{-1}
   \end{equation}
   and hence
   \begin{equation}
     \label{eq:lda:LHtransform}
     \llh \mapsto \Trace{ \mathbf{S} \tmat B \mathbf{S}\T (\mathbf{S}\T)^{-1} \tmat W^{-1} \mathbf{S}^{-1} }
     = \Trace{ \mathbf{S} \tmat B \tmat W^{-1} \mathbf{S}^{-1} }
     = \Trace{\tmat B \tmat W^{-1}}
   \end{equation}
   because of the \href{https://en.wikipedia.org/wiki/Trace_(linear_algebra)#Trace_of_a_product}{similarity invariance of the trace}, which follows from its cyclic property \citep[696, C.9]{Bishop:06}: $\Trace{\mathbf{S} \mathbf{A} \mathbf{S}^{-1}} = \Trace{\mathbf{S}^{-1} \mathbf{S} \mathbf{A}} = \Trace{\mathbf{A}}$ \citep[88]{Deisenroth:Faisal:Ong:20}
 \item this means that only the subspace spanned by $\mathbf{A}$ is relevant, not the specific basis implied; we can thus assume without loss of generality that $\mathbf{A}$ is an orthogonal projection, i.e.\ its rows $\va_k\T$ are orthonormal and $\mathbf{A} \mathbf{A}\T = \mathbf{I}_r$
 \item this enables us to simplify the optimisation problem by sphering $\mathbf{W}$ with the same coordinate transformation $\mathbf{S}$ as in Sec.~\ref{sec:lda:standard:sphering}
   \[
     \mathbf{W}' = \mathbf{S} \mathbf{W} \mathbf{S}\T = \mathbf{I},\qquad
     \mathbf{B}' = \mathbf{S} \mathbf{B} \mathbf{S}\T
   \]
 \item using an orthogonal projection $\mathbf{A}'$ from the transformed coordinates to the discriminant space, eq.~(\ref{eq:lda:tildeWB}) becomes
   \begin{equation}
     \label{eq:lda:tildeWBprime}
     \tmat W' = \mathbf{A}' \mathbf{W}' (\mathbf{A}')\T = \mathbf{A}' (\mathbf{A}')\T = \mathbf{I},\qquad
     \tmat B' = \mathbf{A}' \mathbf{B}' (\mathbf{A}')\T
   \end{equation}
   and the $\llh$ statistic is reduced to
   \begin{equation}
     \label{eq:lda:LHprime}
     \llh(\mathbf{A}') = \Trace{\mathbf{A}' \mathbf{B}' (\mathbf{A}')\T}
     = \sum_{k=1}^r (\va_k')\T \mathbf{B}' \va_k'
   \end{equation}
 \item it stands to reason that $\llh(\mathbf{A}')$ is maximised by the first $r$ eigenvectors $\va_k' = \vv_k$ of $\mathbf{B}'$ and corresponding eigenvalues $\mu_k$ \citep[332]{Venables:Ripley:02}, with $\llh(\mathbf{A}') = \sum_{k=1}^r \mu_k$;%
   \footnote{we will not attempt a more formal proof here, but it should be possible to derive optimality of this solution from the \href{https://en.wikipedia.org/wiki/Low-rank_approximation\#Proof_of_Eckart–Young–Mirsky_theorem_(for_Frobenius_norm)}{Eckart-Young-Mirsky} theorem for the Frobenius norm $\Norm[F]{\mathbf{B}'}$, orthogonal decomposition of the Frobenius norm, and the fact that $\Norm[F]{\mathbf{B}'} = \sum_k \mu_k$.}
 \item discriminant axes in the original coordinate system are obtained as in Sec.~\ref{sec:lda:standard:discriminant} by back-transformation $\va_k = \mathbf{S}\T \va_k'$, or in matrix notation $\mathbf{A} = \mathbf{A}' \mathbf{S}$ (since $\va_k\T = (\va_k')\T \mathbf{S}$)
 \item note that $\mathbf{A}$ is usually not an orthogonal projection after the back-transformation, but can be orthogonalised without affecting the $\llh$ criterion because of (\ref{eq:lda:LHtransform}); our choice of $\mathbf{A}'$ ensures a reasonable scaling of the discriminant space with roughly unit spherical within-group variance%
   \footnote{The coordinate transformation $\mathbf{S}$ ensures that average within-group variance is a unit sphere ($\mathbf{W}' = \mathbf{I}$). Since $\mathbf{A}'$ is chosen to be an orthogonal projection, it preserves the spherical property but reduces variance to the proportion captured by the discriminant space.}
 \item the same solution is also given by \citet[192]{Bishop:06}; a complete (but very condensed) proof based on direct optimisation of $\llh$ and other separation criteria can be found in \citep[446--452]{Fukunaga:90}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Repeated-measures LDA}
\label{sec:lda:repeated}

\begin{itemize}
\item standard LDA aims to minimise within-group variance and maximise between-group variance; but in GMA data points sometimes come from multiple \textbf{cohorts}, whose differences should not affect the discriminant space; a pertinent example is a study of register variation across varieties of English \citep{Neumann:Evert:21}, where the groups to be separated are text categories and cohorts correspond to the different language varieties
\item standard LDA incorporates between-cohort variance in the within-group variance, and thus aims to ``hide'' between-cohort variance in the discriminant space (to minimise within-group variance); on the other hand, group means are averaged across cohorts and possibly reduce between-group variance (if there are differences in the group structure between cohorts)
\item in the example study, the authors' use of standard LDA may thus have actively played down general differences between language varieties (in order to minimise within-group variance) as well as register divergence between varieties (which is averaged out in the between-group variance)
\item it seems more appropriate to treat such cases as a \textbf{repeated-measures design}\footnote{\url{https://en.wikipedia.org/wiki/Repeated_measures_design}} and develop a repeated-measures version of LDA
\end{itemize}

As in Sec.~\ref{sec:lda:anova} we use repeated-measures ANOVA as a starting point, which is a special case of a two-way layout \citep[772-781]{Bishop:06}. Our notation is as follows:
\begin{itemize}
\item data: $n$ observations $y_i\in \setR$ belonging to $g$ groups and $c$ cohorts; $g_i\in \set{1,\ldots,g}$ indicates group membership of $y_i$; $c_i\in \set{1,\ldots, c}$ indicates cohort membership
\item the size of each cell in the two-way layout is given by $n_{jk} = \abs{\set{g_i = j \wedge c_i = k}}$ for group $j$ and cohort $k$; overall group sizes are $n_{j+} = \abs{\set{g_i = j}} = \sum_k n_{jk}$; overall cohort sizes are $n_{+k} = \abs{\set{c_i = k}} = \sum_j n_{jk}$
\item overall mean $m$ as well as the cell means $m_{jk}$, group means $m_{j+}$, and cohort means $m_{+k}$ are given below
  \begin{equation}
    \label{eq:lda:ranova:means}
    \begin{aligned}
      m &= \frac1n \sum_{i=1}^n y_i
      &\quad m_{j+} &= \frac1{n_{j+}} \sum_{g_i = j} y_i = \frac1{n_{j+}} \sum_{k=1}^c n_{jk} m_{jk} \\
      m_{jk} &= \frac1{n_{jk}} \sum_{g_i = j \wedge c_i = k} y_i 
      &\quad m_{+k} &= \frac1{n_{+k}} \sum_{c_i = k} y_i = \frac1{n_{+k}} \sum_{j=1}^g n_{jk} m_{jk} 
    \end{aligned}
  \end{equation}
\item the overall sum of squares $S^2$ can be partitioned into four components
  \begin{equation}
    \label{eq:lda:ranova:ss-partition}
    S^2 = S_g^2 + S^2_c + S^2_{g:c} + S^2_{\text{res}}
  \end{equation}
  where $S^2_{g:c}$ represents the interaction between groups and cohorts and $S^2_{\text{res}}$ is the residual within-cell variance; the five terms are given by
  \begin{equation}
    \label{eq:lda:ranova:ss-terms}
    \begin{split}
      S^2 &= \sum_{i=1}^n (y_i - m)^2 \\
      S^2_g &= \sum_{j=1}^g n_{j+} (m_{j+} - m)^2 \\
      S^2_c &= \sum_{k=1}^c n_{+k} (m_{+k} - m)^2 \\
      S^2_{g:c} &= \sum_{j=1}^g \sum_{k=1}^c n_{jk} (m_{jk} - m_{j+} - m_{+k} + m)^2 \\
      S^2_{\text{res}} &= \sum_{j=1}^g \sum_{k=1}^c \sum_{g_i = j\wedge c_i = k} (y_i - m_{jk})^2 = \sum_{i=1}^n (y_i - m_{g_i c_i})^2
    \end{split}
  \end{equation}
  \citep[775--776]{Bishop:06}
\item various ANOVA hypotheses can be tested by comparing different components of the sum of squares against $S^2_{\text{res}}$, though the resulting ratios are F-scores only for equal cell sizes $n_{jk}$ \citep[777--779]{Bishop:06}
\item we are not interested in differences between cohorts $S^2_c$; the appropriate test is thus for a nested effect of groups within varieties by comparing $S^2_g + S^2_{g:c} = S^2_{c/g}$ against $S^2_{\text{res}}$; in other terms, our ANOVA test partitions the within-cohort variance1
  \[
    S^2 - S^2_c = S^2_{c/g} + S^2_{\text{res}}
  \]
\item the nested sum of squares simplifies to
  \begin{equation}
    \label{eq:lda:ranova:ss-nested}
    S^2_{c/g} = \sum_{j=1}^g \sum_{k=1}^c n_{jk} (m_{jk} - m_{+k})^2 
  \end{equation}
  which can be seen from (\ref{eq:lda:ranova:means}) and (\ref{eq:lda:ranova:ss-terms}):
  \begin{equation*}
    \begin{split}
      S^2_{g:c}
      &= \sum_{j=1}^g \sum_{k=1}^c n_{jk} ((m_{jk} - m_{+k}) - (m_{j+} - m))^2 \\
      &= \underbrace{ \sum_{j=1}^g \sum_{k=1}^c n_{jk} (m_{jk} - m_{+k})^2 }_{S^2_{c/g}}
        + \underbrace{ \sum_{j=1}^g \sum_{k=1}^c n_{jk} (m_{j+} - m)^2 }_{S^2_g}
        - 2 \sum_{j=1}^g \sum_{k=1}^c n_{jk} (m_{jk} - m_{+k})(m_{j+} - m) \\
      &= S^2_{c/g} + S^2_g - 2 \sum_{j=1}^g (m_{j+} - m)
        \underbrace{ \sum_{k=1}^c n_{jk} (m_{jk} - m_{+k}) }_{n_{j+} (m_{j+} - m)} \\
      &= S^2_{c/g} + S^2_g - 2 \sum_{j=1}^g n_{j+} (m_{j+} - m)^2
        = S^2_{c/g} + S^2_g - 2 S^2_g
    \end{split}
  \end{equation*}
\item the corresponding within-nested-group and between-nested-group variances are 
  \begin{equation}
    \label{eq:lda:ranova:WB}
    W = \frac{ S^2_{\text{res}} }{ n - cg } \qquad
    B = \frac{ S^2_{c/g} }{ c(g - 1) }
  \end{equation}
  \citep[778, eq.~(11.8.14)]{Bishop:06}; note that the df add up to $n - c$ for $S^2 - S^2_c$
\end{itemize}

\textbf{Repeated-measures LDA} simply changes the definitions of means and covariance matrices $\mathbf{W}, \mathbf{B}$ from Sec.~\ref{sec:lda:standard} to match eq.~(\ref{eq:lda:ranova:means})--~(\ref{eq:lda:ranova:WB}). All other steps of the algorithm remain valid as described.
\begin{itemize}
\item data are $n$ feature vectors $\vx_i \in \setR^d$ combined into a data matrix $\mathbf{X}\in \setR^{n\times d}$
\item each data point is assigned to one of $g$ groups indicated by $g_i\in \set{1, \ldots, g}$ and one of $c$ cohorts indicated by $c_i\in \set{1, \ldots, c}$
\item the size of each (group, cohort)-cell $(j, k)$ in this two-way layout is given by $n_{jk} = \abs{\set{g_i = j \wedge c_i = k}}$; overall group/cohort sizes are $n_{j+} = \abs{\set{g_i = j}} = \sum_k n_{jk}$ and $n_{+k} = \abs{\set{c_i = k}} = \sum_j n_{jk}$ 
\item overall mean $\vm$ and the means for cells ($\vm_{jk}$), groups ($\vm_{j+}$), and cohorts ($\vm_{+k}$) are given by
  \begin{equation}
    \begin{aligned}
      \label{eq:lda:repeated:means}
      \vm &= \frac1n \sum_{i=1}^n \vx_i 
      &\quad \vm_{j+} &= \frac1{n_{j+}} \sum_{g_i = j} \vx_i = \frac1{n_{j+}} \sum_{k=1}^c n_{jk} \vm_{jk} \\
      \vm_{jk} &= \frac1{n_{jk}} \sum_{g_i = j \wedge c_i = k} \vx_i 
      &\quad \vm_{+k} &= \frac1{n_{+k}} \sum_{c_i = k} \vx_i = \frac1{n_{+k}} \sum_{j=1}^g n_{jk} \vm_{jk}
    \end{aligned}
  \end{equation}
\item nested-within-group and nested-between-group covariance matrices are generalised from eq.~(\ref{eq:lda:ranova:ss-terms}), (\ref{eq:lda:ranova:ss-nested}), (\ref{eq:lda:ranova:WB}) in analogy to (\ref{eq:lda:anova:W}) and (\ref{eq:lda:anova:B})
  \begin{align}
    \label{eq:lda:repeated:W}
    \mathbf{W} &= \frac1{n - cg} \sum_{i=1}^n (\vx_i - \vm_{g_i c_i}) (\vx_i - \vm_{g_i c_i})\T \\
    \label{eq:lda:repeated:B}
    \mathbf{B} &= \frac1{c (g - 1)} \sum_{j=1}^g \sum_{k=1}^c n_{jk} (\vm_{jk} - \vm_{+k}) (\vm_{jk} - \vm_{+k})\T
  \end{align}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Implementation}
\label{sec:lda:implement}

A naive straightforward implementation of LDA consists of the following steps:

\begin{enumerate}
\item Compute between-group variance matrix $\mathbf{B}$ and within-group variance matrix $\mathbf{W}$ according to (\ref{eq:lda:W}) and (\ref{eq:lda:B}).
  \begin{itemize}
  \item let $\mathbf{M}\in \setR^{g\times d}$ the row matrix of group means and $\mathbf{X}_M\in \setR^{n\times d}$ the row matrix containing group means $\vm_{g_i}$ for each data point $\vx_i$
  \item define $\mathbf{X}_W = \mathbf{X} - \mathbf{X}_M$ so that $\mathbf{W} = \frac1{n - g} (\mathbf{X}_W)\T \mathbf{X}_W$
  \item define $\mathbf{X}_B = \mathbf{X}_M - \vone_n \vm\T$ so that $\mathbf{B} = \frac{1}{g - 1} (\mathbf{X}_B)\T \mathbf{X}_B$ (because $\vm_j$ is repeated $n_j$ times)
  \item $\mathbf{B}$ can be computed more efficiently from $\mathbf{M}_B = \Diag{n_1, \ldots, n_g}^{\frac12} (\mathbf{M} - \vone_g \vm\T)$
  \end{itemize}
\item Determine eigenvalue decomposition $\mathbf{W} = \mathbf{U} \mathbf{D} \mathbf{U}\T$ with $\mathbf{D} = \Diag{\lambda_1, \ldots, \lambda_d}$, checking that $\mathbf{W}$ has full rank and a reasonable condition number, i.e.\ $\lambda_d > \epsilon \lambda_1$ (based on \texttt{tol=}).
\item Construct coordinate transformation $\mathbf{S} = \mathbf{D}^{-\frac12} \mathbf{U}\T$ for sphering $\mathbf{W}$. Its inverse is given by $\mathbf{S}^{-1} = \mathbf{U} \mathbf{D}^{\frac12}$, but doesn't seem to be needed by the algorithm.
\item Compute between-group variance matrix $\mathbf{B}' = \mathbf{S} \mathbf{B} \mathbf{S}\T$ in the new coordinate system.
\item Determine eigenvalue decomposition $\mathbf{B}' = \mathbf{V} \mathbf{E} \mathbf{V}\T$ with $\mathbf{E} = \Diag{\mu_1, \mu_2, \ldots}$.
\item Choose number $r$ of discriminant axes such that $r\leq g-1$, $r\leq \Rank{\mathbf{B}'}$ and $\mu_r > \epsilon \mu_1$ (or perhaps some $R^2$-like criterion).
\item Construct orthogonal discriminant projection $\mathbf{A}' = \mathbf{V}_r\T$, then transform to original coordinates $\mathbf{A} = \mathbf{A}' \mathbf{S}$ (or simply $\mathbf{A}\T = \mathbf{S}\T \mathbf{V}_r$ to obtain discriminants as column vectors).
\item Obtain discriminant scores as $\mathbf{Y} = \mathbf{X} \mathbf{A}\T$.
\end{enumerate}

To avoid unnecessary computation and potential rounding errors, it is possible to determine the required eigenvectors of $\mathbf{W}$ and $\mathbf{B}'$ from singular-value decomposition (SVD) of $\mathbf{X}_W$ and $\mathbf{M}_B$ without computing the full covariance matrices:

\begin{itemize}
\item[2.] Compute the SVD $\mathbf{X}_W = \mathbf{U}_W \matSigma_W \mathbf{V}_W\T$. Since
  \[
    \mathbf{W} = \frac1{n - g} (\mathbf{X}_W)\T \mathbf{X}_W
    = \frac1{n - g} \mathbf{V}_W \matSigma_W \mathbf{U}_W\T \mathbf{U}_W \matSigma_W \mathbf{V}_W\T
    = \frac1{n - g} \mathbf{V}_W \matSigma_W^2 \mathbf{V}_W\T
  \]
  its eigenvalue decomposition is given by $\mathbf{U} = \mathbf{V}_W$ and $\mathbf{D}^{\frac12} = \frac1{\sqrt{n - g}} \matSigma_W$
\item[4.] We have
  \[
    \mathbf{B}' = \frac1{g - 1} \mathbf{S} (\mathbf{M}_B)\T \mathbf{M}_B \mathbf{S}\T
     = \frac1{g - 1} (\mathbf{M}_B')\T \mathbf{M}_B'
   \]
   with $\mathbf{M}_B' = \mathbf{M}_B \mathbf{S}\T$
\item[5.] Compute the SVD $\mathbf{M}_B' = \mathbf{U}_{B} \matSigma_{B} \mathbf{V}_B\T$. Since
  \[
    \mathbf{B}' = \frac1{g - 1} (\mathbf{M}_B')\T \mathbf{M}_B'
    = \frac1{g - 1} \mathbf{V}_B \matSigma_B^2 \mathbf{V}_B\T
  \]
  its eigenvalue decomposition is given by $\mathbf{V} = \mathbf{V}_B$ and $\mathbf{E} = \frac1{g - 1} \matSigma_B^2$
\end{itemize}

The LDA implementation \texttt{MASS::lda()} allows users to specify prior probabilities $p_j$ of groups rather than using their distribution in the data (i.e.\ $p_j = n_j / n$). This is easily integrated into our algorithm by setting $n_j = p_j n$. The easiest and most important case are equal group weights, i.e.\ $n_j = n / g$, which also generalises to repeated-measures LDA without complications.

Repeated-measures LDA can easily be implemented now by simply changing the definition of $\mathbf{W}$ and $\mathbf{B}$ according to eq.~(\ref{eq:lda:repeated:W}) and (\ref{eq:lda:repeated:B}).

\begin{itemize}
\item let $\mathbf{M}\in \setR^{cg\times d}$ the row matrix of cell means $\vm_{jk}$, and $\mathbf{M}_{+C} \in \setR^{cg\times d}$ the row matrix containing the cohort meaning $\vm_{+k}$ corresponding to each cell mean $\vm_{jk}$
\item let $\mathbf{X}_M\in \setR^{n\times d}$ the row matrix containing cell means $\vm_{g_i c_i}$ for each data point $\vx_i$
\item define $\mathbf{X}_W = \mathbf{X} - \mathbf{X}_M$ so that $\mathbf{W} = \frac1{n - cg} (\mathbf{X}_W)\T \mathbf{X}_W$
\item define $\mathbf{M}_B = \Diag{n_{11}, \ldots, n_{cg}}^{\frac12} (\mathbf{M} - \mathbf{M}_{+C})$ so that $\mathbf{B} = \frac{1}{c (g - 1)} (\mathbf{M}_B)\T \mathbf{M}_B$
\item for the implementation it might be easier to determine the row matrix $\mathbf{X}_C\in \setR^{n\times d}$ of cohort means $\vm_{+ c_i}$ for each data point $\vx_i$ and set $\mathbf{X}_B = \mathbf{X}_M - \mathbf{X}_C$ so that $\mathbf{B} = \frac{1}{c (g - 1)} (\mathbf{X}_B)\T \mathbf{X}_B$
\end{itemize}

Note that the rank of the discriminant space may be larger with only $r \leq c(g - 1)$ guaranteed.  All other steps of the algorithm remain exactly as above.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{}
\label{sec:A}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\label{sec:A:}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{}
\label{sec:B}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\label{sec:B:}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \renewcommand{\bibsection}{}    % avoid (or change) section heading 
\bibliographystyle{apalike}
\bibliography{stefan-literature,stefan-publications}  

\newpage
\listoftodos

\end{document}
